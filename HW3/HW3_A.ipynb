{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f4de3-3c47-41b7-af24-1da7865954cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Read in the dataset from the Excel file ---\n",
    "df = pd.read_excel('K8.xlsx', header=None, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fc5f6d-b61d-46d7-ae67-034b40fc6198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 1, Train Acc: 1.0000, Test Acc: 0.8333\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 3, Train Acc: 0.9412, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 5, Train Acc: 0.8971, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 9, Train Acc: 0.8971, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 11, Train Acc: 0.8971, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 15, Train Acc: 0.8971, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: uniform, n_neighbors: 21, Train Acc: 0.8971, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 1, Train Acc: 1.0000, Test Acc: 0.8333\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 3, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 5, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 9, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 11, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 15, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: unbalanced, Weighting: distance, n_neighbors: 21, Train Acc: 1.0000, Test Acc: 0.9000\n",
      "Dataset: balanced, Weighting: uniform, n_neighbors: 1, Train Acc: 1.0000, Test Acc: 0.8333\n",
      "Dataset: balanced, Weighting: uniform, n_neighbors: 3, Train Acc: 0.9286, Test Acc: 0.8333\n",
      "Dataset: balanced, Weighting: uniform, n_neighbors: 5, Train Acc: 0.7857, Test Acc: 0.6667\n",
      "Dataset: balanced, Weighting: uniform, n_neighbors: 9, Train Acc: 0.6429, Test Acc: 0.5000\n",
      "Dataset: balanced, Weighting: uniform, n_neighbors: 11, Train Acc: 0.6429, Test Acc: 0.3333\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 14, n_neighbors = 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m clf \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mn, weights\u001b[38;5;241m=\u001b[39mweight)\n\u001b[1;32m     53\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m---> 55\u001b[0m train_score \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m test_score \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mscore(X_test_scaled, y_test)\n\u001b[1;32m     58\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m: balance_type,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighting\u001b[39m\u001b[38;5;124m'\u001b[39m: weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDifference (Train - Test)\u001b[39m\u001b[38;5;124m'\u001b[39m: train_score \u001b[38;5;241m-\u001b[39m test_score\n\u001b[1;32m     65\u001b[0m })\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/base.py:706\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:254\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ArgKminClassMode\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    252\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m    253\u001b[0m     ):\n\u001b[0;32m--> 254\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    257\u001b[0m                 [\n\u001b[1;32m    258\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[idx][np\u001b[38;5;241m.\u001b[39margmax(probas, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    262\u001b[0m             )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:355\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_base.py:808\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    806\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    811\u001b[0m     )\n\u001b[1;32m    813\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    814\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 14, n_neighbors = 15"
     ]
    }
   ],
   "source": [
    "# --- 2. Clean the dataset ---\n",
    "# Replace any '?' with NaN\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# The feature columns are all except the last column.\n",
    "feature_cols = df.columns[:-1]\n",
    "target_col = df.columns[-1]\n",
    "\n",
    "# Convert the feature columns to numeric (floats); non-convertible values become NaN.\n",
    "df[feature_cols] = df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove any row that has NaN in any feature column (this removes the row with '?' values)\n",
    "df.dropna(axis=0, subset=feature_cols, inplace=True)\n",
    "\n",
    "# --- 3. Process the target column ---\n",
    "# Convert the target column to string, lowercase it, and map \"active\" to 1 and \"inactive\" to 0.\n",
    "df[target_col] = df[target_col].astype(str).str.lower().map({'active': 1, 'inactive': 0})\n",
    "# Keep only rows with valid targets (0 or 1)\n",
    "df = df[df[target_col].isin([0, 1])]\n",
    "\n",
    "# --- 4. Create a balanced version of the dataset ---\n",
    "class_counts = df[target_col].value_counts()\n",
    "min_count = class_counts.min()\n",
    "df_balanced = df.groupby(target_col).apply(lambda x: x.sample(min_count, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# --- 5. Define modeling parameters ---\n",
    "weighting_types = ['uniform', 'distance']\n",
    "n_neighbors_list = [1, 3, 5, 9, 11, 15, 21]\n",
    "dataset_types = {'unbalanced': df, 'balanced': df_balanced}\n",
    "\n",
    "results = []  # list to store results\n",
    "\n",
    "# --- 6. Loop over each dataset type and parameter combination ---\n",
    "for balance_type, dataset in dataset_types.items():\n",
    "    # Separate features (all columns except the target) and target (last column)\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset[target_col].values\n",
    "\n",
    "    # Split into training (70%) and test (30%) partitions (using stratification)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize (standardize) the features: fit scaler on training, then transform both sets.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Loop over weighting types and n_neighbors values\n",
    "    for weight in weighting_types:\n",
    "        for n in n_neighbors_list:\n",
    "            clf = KNeighborsClassifier(n_neighbors=n, weights=weight)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            train_score = clf.score(X_train_scaled, y_train)\n",
    "            test_score = clf.score(X_test_scaled, y_test)\n",
    "            \n",
    "            results.append({\n",
    "                'Dataset': balance_type,\n",
    "                'Weighting': weight,\n",
    "                'n_neighbors': n,\n",
    "                'Train Accuracy': train_score,\n",
    "                'Test Accuracy': test_score,\n",
    "                'Difference (Train - Test)': train_score - test_score\n",
    "            })\n",
    "            \n",
    "            # Print the model parameters and corresponding accuracies\n",
    "            print(f\"Dataset: {balance_type}, Weighting: {weight}, n_neighbors: {n}, \"\n",
    "                  f\"Train Acc: {train_score:.4f}, Test Acc: {test_score:.4f}\")\n",
    "\n",
    "# --- 7. Create a summary table of results ---\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(results_df)\n",
    "\n",
    "# --- 8. Identify the best parameter combinations ---\n",
    "best_test = results_df.loc[results_df['Test Accuracy'].idxmax()]\n",
    "largest_diff = results_df.loc[results_df['Difference (Train - Test)'].abs().idxmax()]\n",
    "smallest_diff = results_df.loc[results_df['Difference (Train - Test)'].abs().idxmin()]\n",
    "\n",
    "print(\"\\nBest Test Performance:\")\n",
    "print(best_test)\n",
    "\n",
    "print(\"\\nLargest Difference (between Train and Test performance):\")\n",
    "print(largest_diff)\n",
    "\n",
    "print(\"\\nSmallest Difference (between Train and Test performance):\")\n",
    "print(smallest_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bc1d8-f1df-491b-a816-23ff2737b72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
