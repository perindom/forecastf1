{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f923b-0f89-4765-bca1-6dea613e8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [code]\n",
    "# Install required libraries (if not already installed)\n",
    "# Uncomment and run the following commands if you need to install the libraries:\n",
    "# !pip install opencv-python gym jupyter numpy matplotlib kaleido pandas plotly pyyaml requests seaborn scikit-learn torch imageio\n",
    "\n",
    "# %% [code]\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import imageio\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# %% [code]\n",
    "# Setup virtual display for environments that require rendering (e.g., on Google Colab)\n",
    "try:\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# %% [code]\n",
    "# Helper function to process the raw RGB image from the environment.\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to grayscale and resize it to (160, 240).\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (240, 160))\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# %% [code]\n",
    "# Replay Memory to store experiences for training.\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.stack(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.stack(next_states),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# %% [code]\n",
    "# Define the CNN-based Q-Network using PyTorch.\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # The input is a stack of 4 processed frames\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=5, stride=3)  # output: (64, ? , ?)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Compute the size of the conv output so we can define the first fully connected layer.\n",
    "        # Create a dummy input (batch size=1, channels=4, height=160, width=240)\n",
    "        dummy_input = torch.zeros(1, input_channels, 160, 240)\n",
    "        conv_out = self._get_conv_out(dummy_input)\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_out, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.out = nn.Linear(64, num_actions)\n",
    "    \n",
    "    def _get_conv_out(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return int(np.prod(x.size()[1:]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# %% [code]\n",
    "# Define the DQN Agent using the QNetwork defined above.\n",
    "class Agent_DQN:\n",
    "    def __init__(self, env, memory_capacity=10000, batch_size=32, gamma=0.99, lr=1e-4, target_update=1000):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        self.policy_net = QNetwork(input_channels=4, num_actions=self.num_actions).to(self.device)\n",
    "        self.target_net = QNetwork(input_channels=4, num_actions=self.num_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayMemory(memory_capacity)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 5000  # decay rate for epsilon\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                        np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        if random.random() < eps_threshold:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).unsqueeze(0).to(self.device)  # shape: (1,4,160,240)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.from_numpy(states).to(self.device)         # shape: (B,4,160,240)\n",
    "        actions = torch.from_numpy(actions).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        rewards = torch.from_numpy(rewards).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        next_states = torch.from_numpy(next_states).to(self.device)   # shape: (B,4,160,240)\n",
    "        dones = torch.from_numpy(dones).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        \n",
    "        # Current Q values\n",
    "        state_action_values = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Next state Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            expected_state_action_values = rewards + (1 - dones.float()) * self.gamma * next_state_values\n",
    "        \n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# %% [code]\n",
    "# Utility to stack frames\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode):\n",
    "    \"\"\"\n",
    "    Stack frames for state representation.\n",
    "    Args:\n",
    "        stacked_frames: deque object holding previous frames.\n",
    "        new_frame: processed image frame.\n",
    "        is_new_episode: bool, if true, clear the stack.\n",
    "    Returns:\n",
    "        stacked_state: np.array with shape (4, 160, 240)\n",
    "        stacked_frames: updated deque of frames.\n",
    "    \"\"\"\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((160, 240), dtype=np.float32) for _ in range(4)], maxlen=4)\n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(new_frame)\n",
    "    else:\n",
    "        stacked_frames.append(new_frame)\n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "# %% [code]\n",
    "# Training loop parameters\n",
    "num_episodes = 300       # Adjust based on available compute\n",
    "max_steps = 200          # Max steps per episode (CartPole typically lasts 200 steps)\n",
    "target_update_interval = 1000\n",
    "eval_interval = 20\n",
    "\n",
    "# Create gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Initialize DQN agent\n",
    "agent = Agent_DQN(env)\n",
    "episode_rewards = []\n",
    "\n",
    "# Main training loop\n",
    "stacked_frames = deque(maxlen=4)\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    state = env.reset()\n",
    "    # Render initial frame and process image\n",
    "    frame = process_image(env.render(mode='rgb_array'))\n",
    "    state, stacked_frames = stack_frames(None, frame, True)\n",
    "    \n",
    "    total_reward = 0\n",
    "    for t in range(max_steps):\n",
    "        action = agent.select_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Get next state image\n",
    "        frame_next = process_image(env.render(mode='rgb_array'))\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, frame_next, False)\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        agent.optimize_model()\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if total_steps % agent.target_update == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if episode % eval_interval == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-eval_interval:])\n",
    "        print(f\"Episode {episode} - Average Reward: {avg_reward:.2f}\")\n",
    "        \n",
    "# %% [code]\n",
    "# Plot episode rewards over time\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Rewards over Episodes\")\n",
    "plt.show()\n",
    "\n",
    "# %% [code]\n",
    "# Save the trained model (optional)\n",
    "torch.save(agent.policy_net.state_dict(), \"dqn_cartpole.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e8c237-72f6-4e84-9346-5e62b24eba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we:\n",
    "# 1. Set up the environment and necessary libraries.\n",
    "# 2. Define a helper function `process_image` that converts the rendered image to grayscale and resizes it.\n",
    "# 3. Define the CNN architecture as our Q-network.\n",
    "# 4. Create a replay memory buffer for experience replay.\n",
    "# 5. Build an Agent class (`Agent_DQN`) that encapsulates our DQN training and evaluation routines.\n",
    "# 6. Run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56b0d11-b3b3-49d0-b419-2ff54912b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ad7bab-3253-4b99-b9a2-694a8b79b0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff39bf52ed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72542824-8d52-48d2-81b8-33ed443fb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup virtual display for environments that require rendering (e.g., on Google Colab)\n",
    "try:\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defb7914-2d9f-496c-8724-45af5b00c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process the raw RGB image from the environment.\n",
    "def process_image(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to grayscale and resize it to (160, 240).\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (240, 160))\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeaec7bf-385c-4f21-9b43-c2828d548545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Memory to store experiences for training.\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.stack(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.stack(next_states),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bea9592-7c93-4861-a2a1-3237b85758bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-based Q-Network using PyTorch.\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # The input is a stack of 4 processed frames\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=5, stride=3)  # output: (64, ? , ?)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Compute the size of the conv output so we can define the first fully connected layer.\n",
    "        # Create a dummy input (batch size=1, channels=4, height=160, width=240)\n",
    "        dummy_input = torch.zeros(1, input_channels, 160, 240)\n",
    "        conv_out = self._get_conv_out(dummy_input)\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_out, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.out = nn.Linear(64, num_actions)\n",
    "    \n",
    "    def _get_conv_out(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return int(np.prod(x.size()[1:]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839a6888-781e-493d-ae27-e0eee88797da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN Agent using the QNetwork defined above.\n",
    "class Agent_DQN:\n",
    "    def __init__(self, env, memory_capacity=10000, batch_size=32, gamma=0.99, lr=1e-4, target_update=1000):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update = target_update\n",
    "        \n",
    "        self.policy_net = QNetwork(input_channels=4, num_actions=self.num_actions).to(self.device)\n",
    "        self.target_net = QNetwork(input_channels=4, num_actions=self.num_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayMemory(memory_capacity)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 5000  # decay rate for epsilon\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                        np.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        if random.random() < eps_threshold:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).unsqueeze(0).to(self.device)  # shape: (1,4,160,240)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.from_numpy(states).to(self.device)         # shape: (B,4,160,240)\n",
    "        actions = torch.from_numpy(actions).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        rewards = torch.from_numpy(rewards).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        next_states = torch.from_numpy(next_states).to(self.device)   # shape: (B,4,160,240)\n",
    "        dones = torch.from_numpy(dones).unsqueeze(1).to(self.device)  # shape: (B,1)\n",
    "        \n",
    "        # Current Q values\n",
    "        state_action_values = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Next state Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            expected_state_action_values = rewards + (1 - dones.float()) * self.gamma * next_state_values\n",
    "        \n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd858fc4-07da-4ec5-bea5-46528ba1b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to stack frames\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode):\n",
    "    \"\"\"\n",
    "    Stack frames for state representation.\n",
    "    Args:\n",
    "        stacked_frames: deque object holding previous frames.\n",
    "        new_frame: processed image frame.\n",
    "        is_new_episode: bool, if true, clear the stack.\n",
    "    Returns:\n",
    "        stacked_state: np.array with shape (4, 160, 240)\n",
    "        stacked_frames: updated deque of frames.\n",
    "    \"\"\"\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((160, 240), dtype=np.float32) for _ in range(4)], maxlen=4)\n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(new_frame)\n",
    "    else:\n",
    "        stacked_frames.append(new_frame)\n",
    "    stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdac3f-6a8a-4b16-85aa-942fe8efdc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 - Average Reward: 19.45\n"
     ]
    }
   ],
   "source": [
    "# Training loop parameters\n",
    "num_episodes = 300       # Adjust based on available compute\n",
    "max_steps = 200          # Max steps per episode (CartPole typically lasts 200 steps)\n",
    "target_update_interval = 1000\n",
    "eval_interval = 20\n",
    "\n",
    "# Create gym environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# Initialize DQN agent\n",
    "agent = Agent_DQN(env)\n",
    "episode_rewards = []\n",
    "\n",
    "# Main training loop\n",
    "stacked_frames = deque(maxlen=4)\n",
    "total_steps = 0\n",
    "for episode in range(1, num_episodes+1):\n",
    "    state = env.reset()\n",
    "    # Render initial frame and process image\n",
    "    frame = process_image(env.render())\n",
    "    state, stacked_frames = stack_frames(None, frame, True)\n",
    "    \n",
    "    total_reward = 0\n",
    "    for t in range(max_steps):\n",
    "        action = agent.select_action(state)\n",
    "        _, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Get next state image\n",
    "        frame_next = process_image(env.render())\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, frame_next, False)\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        agent.optimize_model()\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if total_steps % agent.target_update == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    if episode % eval_interval == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-eval_interval:])\n",
    "        print(f\"Episode {episode} - Average Reward: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3037ed-11a8-481f-bea2-317871ba4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode rewards over time\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Rewards over Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218b28a-9617-4dcd-8326-8c13383039f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (optional)\n",
    "torch.save(agent.policy_net.state_dict(), \"dqn_cartpole.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f0be7-ec7a-4875-9413-ec9dc54b1ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
