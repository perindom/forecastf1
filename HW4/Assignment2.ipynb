{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2f9f3a-0ff2-4a80-a698-da0c94f70de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.6/479.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.70.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow) (57.5.0)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.32.3)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.20.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 cachetools-5.5.2 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.38.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.11.0 keras-2.13.1 libclang-18.1.1 markdown-3.7 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 protobuf-4.25.6 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.6 wrapt-1.17.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4f624b2-d640-4de2-abf0-323d02f54298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a custom R² metric for Keras as a proxy for \"accuracy\" in regression.\n",
    "def r2_metric(y_true, y_pred):\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - SS_res/(SS_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    \"\"\"\n",
    "    Reads the Census data, drops unwanted weight columns,\n",
    "    and processes features based on their attribute types.\n",
    "    \"\"\"\n",
    "    # Read data from the Excel file\n",
    "    df = pd.read_excel(filepath)\n",
    "    \n",
    "    # Drop the census weight columns\n",
    "    drop_cols = ['HSUP_WGT', 'MARSUPWT', 'FSUP_WGT']\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=col)\n",
    "    \n",
    "    # Target variable is AGI.\n",
    "    y = df['AGI']\n",
    "    \n",
    "    # Remove target column from features\n",
    "    X = df.drop(columns=['AGI'])\n",
    "    \n",
    "    # Binary and ordinal features are assumed to be already numeric.\n",
    "    # One-hot encode categorical columns: PAW_YN, A_MARITL, PENATVTY.\n",
    "    categorical_cols = ['PAW_YN', 'A_MARITL', 'PENATVTY']\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Ensure all remaining columns are numeric.\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Handle any missing values by dropping rows (or consider imputation if desired)\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def build_model(input_dim, architecture):\n",
    "    \"\"\"\n",
    "    Constructs a Keras Sequential model with the given hidden layer sizes.\n",
    "    Each hidden layer uses ReLU activation, and the output layer is linear.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # First hidden layer with the specified input dimension\n",
    "    model.add(Dense(architecture[0], activation='relu', input_dim=input_dim))\n",
    "    # Additional hidden layers as specified by the architecture tuple\n",
    "    for units in architecture[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "    # Output layer: one neuron for regression (linear activation)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # Compile model using the Adam optimizer, Mean Squared Error loss,\n",
    "    # and track our custom R² metric along with MAE.\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[r2_metric, 'mae'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, architecture):\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = build_model(input_dim, architecture)\n",
    "    \n",
    "    # Set up early stopping; here we monitor the validation loss with min_delta of 0.0005.\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.0005, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model, using 20% of the training data for validation.\n",
    "    model_history = model.fit(X_train, y_train, \n",
    "                        validation_split=0.2, \n",
    "                        epochs=200, \n",
    "                        callbacks=[early_stop], \n",
    "                        verbose=1)\n",
    "    \n",
    "    epochs_trained = len(model_history.history['loss'])\n",
    "    \n",
    "    # Plot training loss (MSE) and validation \"accuracy\" (R² metric) versus epoch.\n",
    "    print(model_history.history.keys())\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(model_history.history['loss'], label='Training Loss (MSE)')\n",
    "    plt.plot(model_history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f\"Training Curves for Architecture {architecture}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"training_curve_{'_'.join(map(str, architecture))}.png\")  # This line saves the figure\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate the model on the full training set and test set.\n",
    "    train_eval = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # The model.evaluate() returns [loss, r2_metric, mae]\n",
    "    train_mse = train_eval[0]\n",
    "    train_r2 = train_eval[1]\n",
    "    train_mae = train_eval[2]\n",
    "    \n",
    "    test_mse = test_eval[0]\n",
    "    test_r2 = test_eval[1]\n",
    "    test_mae = test_eval[2]\n",
    "    \n",
    "    # Define generalization gap as the difference between training and test R² scores.\n",
    "    generalization_gap = train_r2 - test_r2\n",
    "    \n",
    "    # Print the model training information\n",
    "    print(f\"Number of epochs: {epochs_trained}\")\n",
    "    print(\"Training Set Metrics:\")\n",
    "    print(f\"  Coefficient of Determination (R²): {train_r2:.4f}\")\n",
    "    print(f\"  MSE: {train_mse:.4f}\")\n",
    "    print(f\"  MAE: {train_mae:.4f}\")\n",
    "    print(\"Test Set Metrics:\")\n",
    "    print(f\"  Coefficient of Determination (R²): {test_r2:.4f}\")\n",
    "    print(f\"  MSE: {test_mse:.4f}\")\n",
    "    print(f\"  MAE: {test_mae:.4f}\")\n",
    "    print(f\"Generalization Gap (Train R² - Test R²): {generalization_gap:.4f}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'architecture': architecture,\n",
    "        'epochs': epochs_trained,\n",
    "        'train_r2': train_r2,\n",
    "        'train_mse': train_mse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mse': test_mse,\n",
    "        'test_mae': test_mae,\n",
    "        'generalization_gap': generalization_gap\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0c045-6bdf-45d4-8c35-dc8483f3bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Architecture: (4, 4)\n",
      "Epoch 1/200\n",
      "1750/1750 [==============================] - 4s 2ms/step - loss: 9806257152.0000 - r2_metric: -0.2957 - mae: 39237.8750 - val_loss: 9888379904.0000 - val_r2_metric: -0.2701 - val_mae: 38235.3008\n",
      "Epoch 2/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 9333365760.0000 - r2_metric: -0.2068 - mae: 38475.4570 - val_loss: 9080040448.0000 - val_r2_metric: -0.1143 - val_mae: 36958.6719\n",
      "Epoch 3/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 8293796352.0000 - r2_metric: -0.0228 - mae: 36943.5312 - val_loss: 7884699136.0000 - val_r2_metric: 0.0859 - val_mae: 35421.7227\n",
      "Epoch 4/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 7146820608.0000 - r2_metric: 0.1477 - mae: 35878.8438 - val_loss: 6873478144.0000 - val_r2_metric: 0.1960 - val_mae: 35101.4375\n",
      "Epoch 5/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 6384791040.0000 - r2_metric: 0.2032 - mae: 36176.2773 - val_loss: 6319612928.0000 - val_r2_metric: 0.1953 - val_mae: 35980.1836\n",
      "Epoch 6/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 5993512448.0000 - r2_metric: 0.2088 - mae: 36817.0273 - val_loss: 6025784832.0000 - val_r2_metric: 0.1725 - val_mae: 36432.2266\n",
      "Epoch 7/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 5753913856.0000 - r2_metric: 0.1982 - mae: 36874.3789 - val_loss: 5788536320.0000 - val_r2_metric: 0.1666 - val_mae: 36219.3516\n",
      "Epoch 8/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 5537822208.0000 - r2_metric: 0.1882 - mae: 36497.8320 - val_loss: 5559500288.0000 - val_r2_metric: 0.1744 - val_mae: 35654.7188\n",
      "Epoch 9/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 5327431680.0000 - r2_metric: 0.2038 - mae: 35824.6875 - val_loss: 5323722240.0000 - val_r2_metric: 0.1828 - val_mae: 35074.3477\n",
      "Epoch 10/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 5109205504.0000 - r2_metric: 0.1987 - mae: 35164.5820 - val_loss: 5086050816.0000 - val_r2_metric: 0.1953 - val_mae: 34423.4531\n",
      "Epoch 11/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 4895055872.0000 - r2_metric: 0.2339 - mae: 34528.8320 - val_loss: 4844432896.0000 - val_r2_metric: 0.2144 - val_mae: 33661.7617\n",
      "Epoch 12/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 4677153280.0000 - r2_metric: 0.2447 - mae: 33763.9375 - val_loss: 4584430080.0000 - val_r2_metric: 0.2320 - val_mae: 32926.4219\n",
      "Epoch 13/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 4448684544.0000 - r2_metric: 0.2694 - mae: 32998.5664 - val_loss: 4312539136.0000 - val_r2_metric: 0.2505 - val_mae: 32180.7734\n",
      "Epoch 14/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 4217016832.0000 - r2_metric: 0.3134 - mae: 32230.9238 - val_loss: 4045113088.0000 - val_r2_metric: 0.2711 - val_mae: 31406.8359\n",
      "Epoch 15/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 3992517632.0000 - r2_metric: 0.2995 - mae: 31444.6855 - val_loss: 3783630080.0000 - val_r2_metric: 0.2928 - val_mae: 30613.4414\n",
      "Epoch 16/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 3761856000.0000 - r2_metric: 0.3493 - mae: 30630.5684 - val_loss: 3517862656.0000 - val_r2_metric: 0.3140 - val_mae: 29791.8164\n",
      "Epoch 17/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 3538648064.0000 - r2_metric: 0.3603 - mae: 29802.1211 - val_loss: 3263532288.0000 - val_r2_metric: 0.3349 - val_mae: 28944.9844\n",
      "Epoch 18/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 3335735040.0000 - r2_metric: 0.3804 - mae: 28961.6602 - val_loss: 3049662976.0000 - val_r2_metric: 0.3606 - val_mae: 28109.1406\n",
      "Epoch 19/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 3147104256.0000 - r2_metric: 0.4026 - mae: 28110.0176 - val_loss: 2847176448.0000 - val_r2_metric: 0.3820 - val_mae: 27312.1191\n",
      "Epoch 20/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2980744704.0000 - r2_metric: 0.4291 - mae: 27316.9316 - val_loss: 2679645440.0000 - val_r2_metric: 0.4041 - val_mae: 26569.2305\n",
      "Epoch 21/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2835588352.0000 - r2_metric: 0.4455 - mae: 26585.6562 - val_loss: 2533709056.0000 - val_r2_metric: 0.4238 - val_mae: 25876.6797\n",
      "Epoch 22/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2721716480.0000 - r2_metric: 0.4745 - mae: 25965.8086 - val_loss: 2425622016.0000 - val_r2_metric: 0.4457 - val_mae: 25273.1426\n",
      "Epoch 23/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2631533568.0000 - r2_metric: 0.4632 - mae: 25438.1777 - val_loss: 2341537536.0000 - val_r2_metric: 0.4640 - val_mae: 24806.0723\n",
      "Epoch 24/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2560865792.0000 - r2_metric: 0.4928 - mae: 24974.2383 - val_loss: 2275844864.0000 - val_r2_metric: 0.4814 - val_mae: 24394.5156\n",
      "Epoch 25/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2508165376.0000 - r2_metric: 0.5131 - mae: 24634.9219 - val_loss: 2230077184.0000 - val_r2_metric: 0.4953 - val_mae: 24086.7344\n",
      "Epoch 26/200\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 2468252160.0000 - r2_metric: 0.5144 - mae: 24346.2715 - val_loss: 2194484992.0000 - val_r2_metric: 0.5076 - val_mae: 23833.1367\n",
      "Epoch 27/200\n",
      "1718/1750 [============================>.] - ETA: 0s - loss: 2447238656.0000 - r2_metric: 0.5094 - mae: 24165.6816"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Step 1: Read in and preprocess the dataset\n",
    "    X, y = preprocess_data(\"Census_Supplement_Data.xlsx\")\n",
    "    \n",
    "    # Step 2: Split the data into training (70%) and test (30%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Step 3: Normalize the data using StandardScaler (fit on training set only)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Define the list of neural network architectures to try.\n",
    "    architectures = [\n",
    "        (4, 4),\n",
    "        (10, 6),\n",
    "        (32, 16),\n",
    "        (8, 3, 5),\n",
    "        (12, 9, 10)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    # For each architecture, build, train, and evaluate the model.\n",
    "    for arch in architectures:\n",
    "        res = train_and_evaluate_model(X_train, X_test, y_train, y_test, arch)\n",
    "        results.append(res)\n",
    "    \n",
    "    # Display a summary table of results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"Summary of Model Performances:\")\n",
    "    print(results_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb3abc-b2df-4a48-9e3c-2182e27f5ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71aa3a-417b-4590-8d05-22f291b240f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
