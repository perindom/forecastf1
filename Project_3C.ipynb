{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74855b18-4511-4e00-8000-ca7b8acb237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1216a3e9-f70d-474f-9610-151cd0dcc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa73b3b3-ad0f-466a-812a-501189ddb0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/site-packages (from imageio) (10.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from imageio) (1.24.4)\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.35.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d7c5e8-aad6-40ba-b67d-116048a322d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN that outputs Q-values for each action\n",
    "class CartPoleCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(CartPoleCNN, self).__init__()\n",
    "        # Note: The convolutional architecture may need adjustment based on the image dimensions.\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to have shape (batch, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Replay Buffer for storing transitions\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Helper function to preprocess an image from the environment.\n",
    "def preprocess_image(image, device):\n",
    "    # Convert image from HxWxC (RGB) to CxHxW and scale pixel values to [0, 1].\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "    tensor = torch.from_numpy(image).unsqueeze(0).to(device)  # add batch dimension\n",
    "    return tensor\n",
    "\n",
    "def train_dqn():\n",
    "    # Set up device and environment\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize policy and target networks\n",
    "    policy_net = CartPoleCNN(num_actions).to(device)\n",
    "    target_net = CartPoleCNN(num_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()  # target network in inference mode\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.05\n",
    "    epsilon_decay = 500  # decay factor for epsilon\n",
    "    total_steps = 0\n",
    "    num_episodes = 500\n",
    "    target_update_interval = 10  # update target network every N episodes\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and get the initial image state.\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            obs, _ = reset_result\n",
    "        else:\n",
    "            obs = reset_result\n",
    "        # We use the rendered image as our state.\n",
    "        state_img = env.render()\n",
    "        state = preprocess_image(state_img, device)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection.\n",
    "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1.0 * total_steps / epsilon_decay)\n",
    "            total_steps += 1\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(num_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            # Take a step in the environment.\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 5:\n",
    "                _, reward, done, truncated, _ = step_result\n",
    "                done = done or truncated\n",
    "            else:\n",
    "                _, reward, done, _ = step_result\n",
    "\n",
    "            # Get next state as an image and preprocess it.\n",
    "            next_state_img = env.render()\n",
    "            next_state = preprocess_image(next_state_img, device)\n",
    "            \n",
    "            # Save transition in replay buffer.\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Perform training only if we have enough transitions.\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                print(replay_buffer.sample(batch_size))\n",
    "                states_np, actions_np, rewards_np, next_states_np, dones_np = replay_buffer.sample(batch_size)\n",
    "                \n",
    "                # The states and next_states are stored as numpy arrays of tensors.\n",
    "                # We need to concatenate them along the batch dimension.\n",
    "                states = torch.cat(list(states_np)).to(device)  # shape: (batch, C, H, W)\n",
    "                actions = torch.tensor(actions_np, device=device).unsqueeze(1)  # shape: (batch, 1)\n",
    "                rewards = torch.tensor(rewards_np, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "                next_states = torch.cat(list(next_states_np)).to(device)\n",
    "                dones = torch.tensor(dones_np, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "                \n",
    "                # Compute current Q-values from policy network.\n",
    "                current_q_values = policy_net(states).gather(1, actions)\n",
    "                \n",
    "                # Compute target Q-values using the target network.\n",
    "                with torch.no_grad():\n",
    "                    max_next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                    target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "                \n",
    "                # Compute loss and update the policy network.\n",
    "                loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Update the target network every few episodes.\n",
    "        if episode % target_update_interval == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episode {episode} Reward: {episode_reward:.2f} Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    torch.save(policy_net.state_dict(), \"dqn_cartpole.pth\")\n",
    "    print(\"Training finished and model saved as dqn_cartpole.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a537f491-cabe-4f2b-9931-610f4b39a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 13.00 Epsilon: 0.98\n",
      "Episode 1 Reward: 15.00 Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/247220065.py:32: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  state, action, reward, next_state, done = map(np.array, zip(*batch))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Perform training only if we have enough transitions.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     70\u001b[0m     states_np, actions_np, rewards_np, next_states_np, dones_np \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# The states and next_states are stored as numpy arrays of tensors.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# We need to concatenate them along the batch dimension.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 32\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m     31\u001b[0m     batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size)\n\u001b[0;32m---> 32\u001b[0m     state, action, reward, next_state, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, action, reward, next_state, done\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Set up device and environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialize policy and target networks\n",
    "policy_net = CartPoleCNN(num_actions).to(device)\n",
    "target_net = CartPoleCNN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # target network in inference mode\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 500  # decay factor for epsilon\n",
    "total_steps = 0\n",
    "num_episodes = 500\n",
    "target_update_interval = 10  # update target network every N episodes\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and get the initial image state.\n",
    "    reset_result = env.reset()\n",
    "    if isinstance(reset_result, tuple):\n",
    "        obs, _ = reset_result\n",
    "    else:\n",
    "        obs = reset_result\n",
    "    # We use the rendered image as our state.\n",
    "    state_img = env.render()\n",
    "    state = preprocess_image(state_img, device)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection.\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1.0 * total_steps / epsilon_decay)\n",
    "        total_steps += 1\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        # Take a step in the environment.\n",
    "        step_result = env.step(action)\n",
    "        if len(step_result) == 5:\n",
    "            _, reward, done, truncated, _ = step_result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            _, reward, done, _ = step_result\n",
    "\n",
    "        # Get next state as an image and preprocess it.\n",
    "        next_state_img = env.render()\n",
    "        next_state = preprocess_image(next_state_img, device)\n",
    "        \n",
    "        # Save transition in replay buffer.\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Perform training only if we have enough transitions.\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            print(replay_buffer.sample(batch_size))\n",
    "            states_np, actions_np, rewards_np, next_states_np, dones_np = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            # The states and next_states are stored as numpy arrays of tensors.\n",
    "            # We need to concatenate them along the batch dimension.\n",
    "            states = torch.cat(list(states_np)).to(device)  # shape: (batch, C, H, W)\n",
    "            actions = torch.tensor(actions_np, device=device).unsqueeze(1)  # shape: (batch, 1)\n",
    "            rewards = torch.tensor(rewards_np, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.cat(list(next_states_np)).to(device)\n",
    "            dones = torch.tensor(dones_np, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            # Compute current Q-values from policy network.\n",
    "            current_q_values = policy_net(states).gather(1, actions)\n",
    "            \n",
    "            # Compute target Q-values using the target network.\n",
    "            with torch.no_grad():\n",
    "                max_next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "            \n",
    "            # Compute loss and update the policy network.\n",
    "            loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update the target network every few episodes.\n",
    "    if episode % target_update_interval == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(f\"Episode {episode} Reward: {episode_reward:.2f} Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "env.close()\n",
    "torch.save(policy_net.state_dict(), \"dqn_cartpole.pth\")\n",
    "print(\"Training finished and model saved as dqn_cartpole.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "887c826b-77be-4f1c-ae7f-db4ad5bfcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(replay_buffer.buffer[0][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31b6e36c-f9a6-4241-8998-6e3dfe902c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_img = np.transpose(next_state_img, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "tensor = torch.from_numpy(next_state_img).unsqueeze(0).to(device)  # add batch dimension\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c55745b4-363d-4870-8c06-266cfd726785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that runs the simulation, collects frames, and displays a GIF\n",
    "def run_cartpole_cnn_gif():\n",
    "    # Set up device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create the CartPole environment with render_mode 'rgb_array'\n",
    "    env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the CNN model\n",
    "    model = CartPoleCNN(num_actions).to(device)\n",
    "    \n",
    "    # Optionally, load a pretrained model:\n",
    "    # model.load_state_dict(torch.load(\"cartpole_cnn.pth\"))\n",
    "    \n",
    "    frames = []  # List to store frames for the GIF\n",
    "    reset_result = env.reset()\n",
    "    \n",
    "    # For gym versions >=0.26, reset returns (observation, info)\n",
    "    if isinstance(reset_result, tuple):\n",
    "        obs, info = reset_result\n",
    "    else:\n",
    "        obs = reset_result\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Capture the current frame from the environment\n",
    "        frame = env.render()  # returns an RGB numpy array\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Use the CNN to predict an action from the image\n",
    "        action = get_action_from_image(model, frame, device)\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        step_result = env.step(action)\n",
    "        # Handle different return signatures for gym versions\n",
    "        if len(step_result) == 5:\n",
    "            obs, reward, done, truncated, info = step_result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            obs, reward, done, info = step_result\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "    env.close()\n",
    "    print(\"Episode finished with total reward:\", total_reward)\n",
    "    \n",
    "    # Save the collected frames as a GIF\n",
    "    gif_filename = 'cartpole_simulation.gif'\n",
    "    # Adjust fps (frames per second) as needed for your simulation speed\n",
    "    imageio.mimsave(gif_filename, frames, fps=3)\n",
    "    \n",
    "    # Display the GIF inline in the Jupyter Notebook\n",
    "    display(Image(filename=gif_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1639a188-ed77-426d-8862-1d7c95e4a89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished with total reward: 10.0\n"
     ]
    },
    {
     "data": {
      "image/gif": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_cartpole_cnn_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3e71ac1-ae09-4c57-ad7e-7909177ab8db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input, Conv2D, Flatten, Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "import cv2\n",
    "\n",
    "class CartPoleAgent:\n",
    "    def __init__(self):\n",
    "        self.model = self.build_cnn_model()\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"\n",
    "        Builds a CNN with the following architecture:\n",
    "        \n",
    "        Input: (4, 160, 240)  [channels_first]\n",
    "        Conv2D: 64 filters, kernel_size=5, strides=(3,3), relu activation -> Output: (64, 52, 79)\n",
    "        Conv2D: 64 filters, kernel_size=4, strides=(2,2), relu activation -> Output: (64, 25, 38)\n",
    "        Conv2D: 64 filters, kernel_size=3, strides=(1,1), relu activation -> Output: (64, 23, 36)\n",
    "        Flatten: -> 52992 units\n",
    "        Dense: 512 units, relu activation\n",
    "        Dense: 256 units, relu activation\n",
    "        Dense: 64 units, relu activation\n",
    "        Dense: 2 units, softmax activation\n",
    "        \"\"\"\n",
    "        # Note: Using channels_first (i.e. (channels, height, width))\n",
    "        input_shape = (4, 160, 240)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # First convolutional layer\n",
    "        x = Conv2D(64, kernel_size=5, strides=(3, 3), activation='relu',\n",
    "                   data_format='channels_first')(inputs)  # Output: (64, 52, 79)\n",
    "        # Second convolutional layer\n",
    "        x = Conv2D(64, kernel_size=4, strides=(2, 2), activation='relu',\n",
    "                   data_format='channels_first')(x)       # Output: (64, 25, 38)\n",
    "        # Third convolutional layer\n",
    "        x = Conv2D(64, kernel_size=3, strides=(1, 1), activation='relu',\n",
    "                   data_format='channels_first')(x)       # Output: (64, 23, 36)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = Flatten()(x)                                  # 52992 units\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        outputs = Dense(2, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name=\"cnn model\")\n",
    "        return model\n",
    "    \n",
    "    def train_classifier(self, X_train, y_train, batch_size=32, epochs=10):\n",
    "        \"\"\"\n",
    "        Trains the CNN classifier using the provided training data.\n",
    "        Training will run on GPU if available.\n",
    "        \n",
    "        Parameters:\n",
    "            X_train: Training images with shape (num_samples, 4, 160, 240)\n",
    "            y_train: One-hot encoded labels with shape (num_samples, 2)\n",
    "            batch_size: Batch size for training\n",
    "            epochs: Number of epochs to train\n",
    "        \"\"\"\n",
    "        # Determine if a GPU is available and set the appropriate device.\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            device = '/GPU:0'\n",
    "        else:\n",
    "            device = '/CPU:0'\n",
    "        \n",
    "        # Training within the device scope to leverage GPU acceleration if possible\n",
    "        with tf.device(device):\n",
    "            self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    def process_rendered_image(self, image):\n",
    "        \"\"\"\n",
    "        Compresses the rendered image by converting it to grayscale (if necessary)\n",
    "        and resizing it to 240x160.\n",
    "        \n",
    "        Parameters:\n",
    "            image: The input image from the environment.\n",
    "            \n",
    "        Returns:\n",
    "            The processed image.\n",
    "        \"\"\"\n",
    "        # If the image has three channels (e.g., BGR), convert it to grayscale.\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the image to (240, 160)\n",
    "        image = cv2.resize(image, (240, 160))\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa6f0c9c-5a69-42b4-b95f-2b3d25817a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "497cdebe-c5ce-48ba-afa2-5b885e490471",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimageio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import cv2\n",
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # Input shape: (batch, 4, 160, 240)\n",
    "        # First Conv2D: 64 filters, kernel_size=5, stride=3 -> output: (64, 52, 79)\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=5, stride=3)\n",
    "        # Second Conv2D: 64 filters, kernel_size=4, stride=2 -> output: (64, 25, 38)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Third Conv2D: 64 filters, kernel_size=3, stride=1 -> output: (64, 23, 36)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        # Flatten size = 64 * 23 * 36 = 52992\n",
    "        self.fc1 = nn.Linear(64 * 23 * 36, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)  # 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x))   # -> (batch, 64, 52, 79)\n",
    "        x = F.relu(self.conv2(x))   # -> (batch, 64, 25, 38)\n",
    "        x = F.relu(self.conv3(x))   # -> (batch, 64, 23, 36)\n",
    "        x = x.view(x.size(0), -1)   # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # Output logits; for classification, use CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "class CartPoleAgent:\n",
    "    def __init__(self):\n",
    "        # Set device to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CNNClassifier().to(self.device)\n",
    "    \n",
    "    def train_classifier(self, X_train, y_train, batch_size=32, epochs=10, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Trains the CNN classifier using the provided training data.\n",
    "        \n",
    "        Parameters:\n",
    "            X_train: Training images of shape (num_samples, 4, 160, 240) (numpy array or torch tensor)\n",
    "            y_train: One-hot encoded labels of shape (num_samples, 2) (numpy array or torch tensor)\n",
    "                     (they will be converted to class indices)\n",
    "            batch_size: Batch size for training\n",
    "            epochs: Number of epochs\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        # Convert to torch tensors if necessary\n",
    "        if isinstance(X_train, np.ndarray):\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        if isinstance(y_train, np.ndarray):\n",
    "            y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        \n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        if y_train.ndim == 2 and y_train.size(1) == 2:\n",
    "            y_train = torch.argmax(y_train, dim=1)\n",
    "        \n",
    "        dataset = TensorDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * batch_X.size(0)\n",
    "            epoch_loss = running_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    def process_rendered_image(self, image):\n",
    "        \"\"\"\n",
    "        Converts the rendered image to grayscale (if needed) and resizes it to 240x160.\n",
    "        \n",
    "        Parameters:\n",
    "            image: Input image from the environment.\n",
    "        \n",
    "        Returns:\n",
    "            Processed image.\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.resize(image, (240, 160))\n",
    "        return image\n",
    "\n",
    "def run_episode_and_generate_gif(agent, env_name='CartPole-v1', gif_filename='cartpole_episode.gif'):\n",
    "    \"\"\"\n",
    "    Runs one episode of the CartPole environment using a random policy,\n",
    "    processes each rendered frame with the agent's image compressor,\n",
    "    and saves the episode as a GIF.\n",
    "    \n",
    "    Parameters:\n",
    "        agent: Instance of CartPoleAgent.\n",
    "        env_name: Name of the gym environment.\n",
    "        gif_filename: Filename for the saved GIF.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    obs = env.reset()\n",
    "    frames = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Render the environment and process the image\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        processed_frame = agent.process_rendered_image(frame)\n",
    "        frames.append(processed_frame)\n",
    "        \n",
    "        # Select a random action (replace with your policy if available)\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    env.close()\n",
    "    imageio.mimsave(gif_filename, frames, fps=30)\n",
    "    print(f\"Saved GIF to {gif_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = CartPoleAgent()\n",
    "    \n",
    "    # Optionally, prepare and train your classifier with training data:\n",
    "    # Example:\n",
    "    # X_train = np.random.rand(100, 4, 160, 240)  # Dummy data\n",
    "    # y_train = np.eye(2)[np.random.randint(0, 2, size=(100,))]  # Dummy one-hot labels\n",
    "    # agent.train_classifier(X_train, y_train, batch_size=16, epochs=5)\n",
    "    \n",
    "    # Run an episode and generate a GIF of the resulting episode.\n",
    "    run_episode_and_generate_gif(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43204c05-fccd-43ab-b569-39fa739bb430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089c486-c82b-48ab-9781-f59d5e0fb549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3e5d4-2748-4409-aa34-4be1f7b9ba6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a9a92-ee16-4ceb-9736-fbb64be81f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
